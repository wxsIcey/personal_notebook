# SWA (Sliding Window Attention) 分析

## SWA 对 KV Cache 的关键影响

- Cache大小恒定：每一层的KV Cache不再随序列长度增长而无限增大，大小被限制在窗口W
- Cache需要逐层管理：每一层都独立根据当前生成的位置和窗口规则来管理自己的Cache，决定哪些历史 KV需要被保留，哪些被丢弃
- Prefile阶段也需要掩码：虽然Prefill是并行计算，但仍需遵守SWA的语义，通过掩码确保每个token只关注窗口内的其他token
- 解码性能提升：由于每次Attention计算只需要读取O(W)大小的KV Cache，计算量减少，解码速度会更快

## 具体实例

假设 prompt（A, B, C）和需要生成的序列（D, E, F），Decoder 为 3 层，滑动窗口大小为2，演示 SWA 的过程。

### prefill 阶段

Layer 1:

- 接收 [A, B, C] 的嵌入向量
- 为 A, B, C 同时计算其 Key 和 Value：K₁ᴬ, V₁ᴬ, K₁ᴮ, V₁ᴮ, K₁ᶜ, V₁ᶜ
- 进行Self-Attention计算， A 只能看到自己， B 可以看到 A 和 B， C可以看到 B 和 C，也就是 Q₁ᴬ只与 K₁ᴬ 计算点积，再与 V 进行加权，B 和 C 的注意力计算可以类推
- 输出Attention的结果 [Attn_OUT₁ᴬ, Attn_OUT₁ᴮ, Attn_OUT₁ᶜ]
- 将 (K₁ᴬ, V₁ᴬ), (K₁ᴮ, V₁ᴮ), (K₁ᶜ, V₁ᶜ) 存入Layer 1的Cache
- 对每个位置独立应用 FFN 公式 FFN(x) = max(0, xW₁ + b₁)W₂ + b₂, 得到新的隐藏状态 [H₁ᴬ, H₁ᴮ, H₁ᶜ]

Layer 2:

- 接收Layer 1的输出 [H₁ᴬ, H₁ᴮ, H₁ᶜ] 作为输入
- 为这些输入同时计算其Key和Value：K₂ᴬ, V₂ᴬ, K₂ᴮ, V₂ᴮ, K₂ᶜ, V₂ᶜ
- 进行Attention计算， 和Layer1类似
- 输出Attention结果
- 将 (K₂ᴬ, V₂ᴬ), (K₂ᴮ, V₂ᴮ), (K₂ᶜ, V₂ᶜ) 存入Layer 2的Cache
- 应用FFN变换，输出[H₂ᴬ, H₂ᴮ, H₂ᶜ]

Layer 3:

- 接收Layer 2的输出 [H₂ᴬ, H₂ᴮ, H₂ᶜ] 作为输入
- 为这些输入同时计算其Key和Value：K₃ᴬ, V₃ᴬ, K₃ᴮ, V₃ᴮ, K₃ᶜ, V₃ᶜ
- 进行Attention计算，和Layer 1 类似
- 输出Attention结果
- 将 (K₃ᴬ, V₃ᴬ), (K₃ᴮ, V₃ᴮ), (K₃ᶜ, V₃ᶜ) 存入Layer 3的Cache
- 输出最终隐藏状态 [H₃ᴬ, H₃ᴮ, H₃ᶜ]

生成准备：取最后一个token C 对应的最终输出 H₃ᶜ，通过LM Head（分类器）计算概率分布，采样得到第一个生成 token “D”

### decode 阶段 （只看 Token D， E 和 F 是类似的）

输入： 上一步刚生成的token D

Layer 1:

- 计算 [D] 的 Q₁ᴰ K₁ᴰ V₁ᴰ
- 从Layer 1的Cache中读取Key/Value， 由于窗口 W 为 2， D 只需要与(K₁ᶜ, V₁ᶜ)和(K₁ᴰ, V₁ᴰ) 进行计算
- 将 Q₁ᴰ 与 [K₁ᶜ, K₁ᴰ] 进行点积计算，得到注意力分数，然后对注意力分数应用softmax，得到注意力权重，使用注意力权重对 V 即 [V₁ᶜ, V₁ᴰ] 进行加权求和，得到Attention输出
- 输出Attention结果： Attn_OUT₁ᴰ
- 更新Cache，将新计算的 (K₁ᴰ, V₁ᴰ) 追加到Layer 1的Cache中，同时安全地丢弃位置小于3-2+1=2的 KV，即B和B之前的KV，(K₁ᴬ, V₁ᴬ)、(K₁ᴮ, V₁ᴮ)
- 仍然动用整个FFN的全部参数，计算 FFN(Attn_OUT₁ᴰ)，得到新的隐藏状态H₁ᴰ
  
Layer 2:

- 接收Layer 1的输出 H₁ᴰ 作为输入
- 计算 H₁ᴰ 的 Q₂ᴰ K₂ᴰ V₂ᴰ
- 从Layer 2的Cache中读取历史Key/Value：(K₂ᴬ, V₂ᴬ), (K₂ᴮ, V₂ᴮ), (K₂ᶜ, V₂ᶜ)
- 将 Q₂ᴰ 与 [K₂ᶜ, K₂ᴰ] 进行Attention计算（类似Layer 1)
- 输出Attention结果
- 将新计算的 (K₂ᴰ, V₂ᴰ) 追加到Layer 2的Cache中，并丢弃位置小于3-2+1=2的 KV，即B和B之前的KV，(K₂ᴬ, V₂ᴬ), (K₂ᴮ, V₂ᴮ)
- 对Attention结果动用整个FFN的全局参数，输出H₂ᴰ

Layer 3:

- 接收Layer 2的输出 H₂ᴰ 作为输入
- 计算 H₂ᴰ 的 Q₃ᴰ
- 从Layer 3的Cache中读取所有历史Key/Value：(K₃ᴬ, V₃ᴬ), (K₃ᴮ, V₃ᴮ), (K₃ᶜ, V₃ᶜ)
- 将 Q₃ᴰ 与 [K₃ᴬ, K₃ᴮ, K₃ᶜ, K₃ᴰ] 进行Attention计算(类似Layer 1)
- 输出Attention结果
- 将新计算的 (K₃ᴰ, V₃ᴰ) 追加到Layer 3的Cache中，并丢弃位置小于3-2+1=2的 KV，即B和B之前的KV，(K₃ᴬ, V₃ᴬ), (K₃ᴮ, V₃ᴮ)
- 对Attention结果动用整个FFN的全局参数，输出H₃ᴰ

生成结果：将 H₃ᴰ 通过 LM Head 计算概率分布，采样得到下一个token E
