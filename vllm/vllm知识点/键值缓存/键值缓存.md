# KV Cache分析

## 每层都需要自己的KV Cache

KV Cache是以 Layer 为单位进行存储和管理的，每一层都有自己独立的 KV Cache，用于缓存由该层特有隐藏状态计算出的 Key 和 Value 向量。

1. 隐藏状态变化：当一个输入序列流过解码器时，每一层都会对输入进行变换，输出一个新的、更抽象的隐藏状态表示。第 1 层的输出是第 2 层的输入；
2. 不同层的 token 的 K 和 V 不同：由于每一层的输入隐藏状态都不同，所以即使对于同一个token，它在第1层计算出的K/V和第2层计算出的K/V也是完全不同的。它们代表了该token在不同抽象层级上的“键”和“值”信息。

## 具体实例

假设 prompt（A, B, C）和需要生成的序列（D, E, F），Decoder 为 3 层，演示 KV Cache 的过程。

### prefill 阶段

Layer 1:

- 接收 [A, B, C] 的嵌入向量
- 为 A, B, C 同时计算其 Key 和 Value：K₁ᴬ, V₁ᴬ, K₁ᴮ, V₁ᴮ, K₁ᶜ, V₁ᶜ
- 进行Self-Attention（每个 token 的 Q 与所有 token 的 K/V 作用）（当计算一个单词与其它单词之间关系时，用该单词的Q去请求其他单词的K，得到一个分数，将分数归一化，与每个单词的V做线性加权和得到该单词的self-attention的输出）
- 输出Attention的结果 [Attn_OUT₁ᴬ, Attn_OUT₁ᴮ, Attn_OUT₁ᶜ]
- 将 (K₁ᴬ, V₁ᴬ), (K₁ᴮ, V₁ᴮ), (K₁ᶜ, V₁ᶜ) 存入Layer 1的Cache
- 对每个位置独立应用 FFN 公式 FFN(x) = max(0, xW₁ + b₁)W₂ + b₂, 得到新的隐藏状态 [H₁ᴬ, H₁ᴮ, H₁ᶜ]

Layer 2:

- 接收Layer 1的输出 [H₁ᴬ, H₁ᴮ, H₁ᶜ] 作为输入
- 为这些输入同时计算其Key和Value：K₂ᴬ, V₂ᴬ, K₂ᴮ, V₂ᴮ, K₂ᶜ, V₂ᶜ
- 进行Attention计算
- 输出Attention结果
- 将 (K₂ᴬ, V₂ᴬ), (K₂ᴮ, V₂ᴮ), (K₂ᶜ, V₂ᶜ) 存入Layer 2的Cache
- 应用FFN变换，输出[H₂ᴬ, H₂ᴮ, H₂ᶜ]

Layer 3:

- 接收Layer 2的输出 [H₂ᴬ, H₂ᴮ, H₂ᶜ] 作为输入
- 为这些输入同时计算其Key和Value：K₃ᴬ, V₃ᴬ, K₃ᴮ, V₃ᴮ, K₃ᶜ, V₃ᶜ
- 进行Attention计算
- 输出Attention结果
- 将 (K₃ᴬ, V₃ᴬ), (K₃ᴮ, V₃ᴮ), (K₃ᶜ, V₃ᶜ) 存入Layer 3的Cache
- 输出最终隐藏状态 [H₃ᴬ, H₃ᴮ, H₃ᶜ]

生成准备：取最后一个token C 对应的最终输出 H₃ᶜ，通过LM Head（分类器）计算概率分布，采样得到第一个生成 token “D”

### decode 阶段 （只看 Token D， E 和 F 是类似的）

输入： 上一步刚生成的token D

Layer 1:

- 计算 [D] 的 Q₁ᴰ K₁ᴰ V₁ᴰ
- 从Layer 1的Cache中读取所有历史Key/Value：(K₁ᴬ, V₁ᴬ), (K₁ᴮ, V₁ᴮ), (K₁ᶜ, V₁ᶜ) (注意这是prefile阶段的缓存，这里才是缓存的真正作用！！！)
- 将 Q₁ᴰ 与 [K₁ᴬ, K₁ᴮ, K₁ᶜ, K₁ᴰ] 进行点积计算，得到注意力分数，然后对注意力分数应用softmax，得到注意力权重，使用注意力权重对所有的V即 [V₁ᴬ, V₁ᴮ, V₁ᶜ, V₁ᴰ] 进行加权求和，得到Attention输出。
- 输出Attention结果： Attn_OUT₁ᴰ
- 将新计算的 (K₁ᴰ, V₁ᴰ) 追加到Layer 1的Cache中
- 仍然动用整个FFN的全部参数，计算 FFN(Attn_OUT₁ᴰ)，得到新的隐藏状态H₁ᴰ
  
Layer 2:

- 接收Layer 1的输出 H₁ᴰ 作为输入
- 计算 H₁ᴰ 的 Q₂ᴰ K₂ᴰ V₂ᴰ
- 从Layer 2的Cache中读取所有历史Key/Value：(K₂ᴬ, V₂ᴬ), (K₂ᴮ, V₂ᴮ), (K₂ᶜ, V₂ᶜ)
- 将 Q₂ᴰ 与 [K₂ᴬ, K₂ᴮ, K₂ᶜ, K₂ᴰ] 进行Attention计算（类似Layer 1)
- 输出Attention结果
- 将新计算的 (K₂ᴰ, V₂ᴰ) 追加到Layer 2的Cache中
- 对Attention结果动用整个FFN的全局参数，输出H₂ᴰ

Layer 3:

- 接收Layer 2的输出 H₂ᴰ 作为输入
- 计算 H₂ᴰ 的 Q₃ᴰ
- 从Layer 3的Cache中读取所有历史Key/Value：(K₃ᴬ, V₃ᴬ), (K₃ᴮ, V₃ᴮ), (K₃ᶜ, V₃ᶜ)
- 将 Q₃ᴰ 与 [K₃ᴬ, K₃ᴮ, K₃ᶜ, K₃ᴰ] 进行Attention计算(类似Layer 1)
- 输出Attention结果
- 将新计算的 (K₃ᴰ, V₃ᴰ) 追加到Layer 3的Cache中
- 对Attention结果动用整个FFN的全局参数，输出H₃ᴰ

生成结果：将 H₃ᴰ 通过 LM Head 计算概率分布，采样得到下一个token E。

