# DP 问题

## Local Batch Size < DP Size

Global Batch Size = Local Batch Size × DP Size
Global Batch Size： 整个训练一步（一个优化步骤）所处理的总样本数;
Local Batch Size / Per-GPU Batch Size： 每个GPU在每个训练步上独立处理的样本数;
DP Size： 参与数据并行（Data Parallel, DP）的GPU数量;

### DP workflow

1. 分发： 主GPU获取一个大小为 (Local Batch Size * DP Size) 的全局批次数据;
2. 拆分： 将这个全局批次均匀地拆分成 DP Size 份，每份大小为 Local Batch Size，然后分发到各个GPU上;
3. 前向传播： 每个GPU用自己的模型副本独立处理分到的数据，计算损失；
4. 反向传播： 每个GPU独立计算相对于自己本地数据的梯度；
5. 梯度同步：所有GPU通过 All-Reduce 操作（通常是求和或平均）将各自的梯度同步起来，得到所有数据上平均后的全局梯度；
6. 每个GPU用这个全局梯度同步地更新自己的模型参数；

### issue analysize

假设你有 4 个GPU（DP Size = 4），但你设置每个GPU只处理 2 个样本（Local Batch Size = 2），假设数据集有10个样本，那么第一个全局批次为8个样本，每份2个样本，训练正常，第二个全局批次就只剩下2个样本。

DataLoader现在必须把最后2个样本分给4个GPU，可能就有以下分配：

- GPU 0： 可能得到 2 个样本;
- GPU 1： 可能得到 0 个样本（空）;
- GPU 2： 可能得到 0 个样本（空）;
- GPU 3： 可能得到 0 个样本（空）;
  
GPU 0 正常进行了前向和反向传播，计算出了基于2个样本的梯度,GPU 1, 2, 3 没有数据，它们的梯度张量是空的（None）或者未定义的，当进行 All-Reduce 操作时，框架（如PyTorch）试图去同步4个GPU的梯度，它会尝试对“空”和“有效张量”进行求和操作，这会导致运行时错误。

### issue solve

1. 设置Local Batch Size远大于DP Size
2. 丢弃尾部批次
3. 使用支持自动处理不平衡数据的分布式框架
   Pytorch的DistributedDataParallel（DDP）遇到最后一个批次不完整时，可以自动通过一个额外的通信步骤来创建一个“非对称”通信组，只让有数据的GPU参与All-Reduce，从而避免错误。
