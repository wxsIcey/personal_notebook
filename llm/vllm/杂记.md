# 并行策略

## 数据并行 (Data Parallelism)
数据并行将训练数据划分为多个批次，将每个批次分配给不同的计算卡进行并行处理，每张计算卡上存在一份完整的模型权重副本。

## 张量并行 (Tensor Parallelism)
张量并行是将张量分解为多个小张量，对这些小张量进行并行计算，可以显著减少计算时间和内存占用，提高计算效率。在计算过程中，张量被列切分，然后在不同设备上分配不同的列张量，最后在计算完成后对结果进行拼接。

## 流水线并行 (Pipeline Parallelism)
流水线并行是将同一个任务分成多个阶段，每个阶段由不同的处理器处理，然后将结果传递给下一个阶段，以实现并行计算。

## 序列并行 (Sequence Parallelism)



# 程序执行方式

## Eager Mode
- 逐行执行：代码像普通Python程序一样立即执行，操作直接返回结果；
- 动态计算图：每次运算动态构建计算图，适合调试和交互式开发；
- 灵活性：支持Python原生控制流（如if、for），变量可随时修改；

## Graph Mode
- 静态计算图：先定义完整的计算图，再执行计算（类似“先编译后运行”）；
- 延迟执行：定义图时不实际计算，调用session.run()时才执行（TensorFlow 1.x风格）；
- 优化空间：框架可对图进行剪枝、融合操作等优化，提升性能；

